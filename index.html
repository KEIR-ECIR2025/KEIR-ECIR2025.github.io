<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .profile-image {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            object-fit: cover;
            margin-right: 10px;
            vertical-align: middle;
        }
        .profile-link {
            display: flex;
            align-items: center;
            text-decoration: none;
            color: black;
        }
        @media (max-width: 600px) {
            .profile-link {
                flex-direction: column;
                align-items: flex-start;
            }
            .profile-image {
                margin-bottom: 10px;
            }
        }
    </style>
    <style>
        .navigation {
            width: 100%;
            text-align: center;
            border-collapse: collapse;
        }
        .navigation td {
            padding: 10px 20px; /* Adjust padding as needed */
        }
        .navigation a {
            text-decoration: none;
            color: black;
            font-size: 16px;
            font-weight: bold;
        }
        .navigation a:hover {
            color: goldenrod;
        }
    </style>
    <title>Knowledge-Enhanced Information Retrieval [KEIR 2025]</title>
</head>


<body>

    <a id="home"><div class="banner">
        <img src="assets/background.jpg" alt="Conference Template Banner" width="1000px">
        <div class="top-center">
            <span class="title1">Knowledge-Enhanced Information Retrieval</span><br/>
        </div>
	<div class="top-left">
	  <span class="title2"></span> <span class="year">KEIR 2025</span>
	</div>
        <div class="bottom-right">
            April 10, 2025 @ ECIR <br> Lucca, Italy
        </div>
    </div></a>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href="index.html#">Home</a>
            </td>
            <!-- <td class="navigation">
                <a title="Register for the Conference" href="https://sigir-2024.github.io/attend_registration.html">Registration (SIGIR 2024)</a>
            </td> -->
            <td class="navigation">
                <a title="Conference Program" href="index.html#program">Program</a> 
            </td>
            <td class="navigation">
	      <a title="Speakers" href="index.html#speakers">Speakers</a>
	    </td>
            <td class="navigation">
	      <a title="Panel" href="index.html#panel">Panel</a>
	    </td>
	    <td class="navigation">
                <a title="Organizers" href="index.html#organizers">Organizers</a>
            </td>
        </tr>
    </table>

    <h2>Abstract</h2>
    <p style="margin: 0; padding: 0;">
    Pretrained language models (PLMs) like BERT and GPT-4 have become foundational to modern information retrieval (IR) systems. However, existing PLM-based IR models primarily rely on knowledge learned during training for predictions, limiting their ability to access and incorporate external, up-to-date, or domain-specific information. Consequently, current IR systems struggle with semantic nuances, contextual relevance, and domain-specific challenges.</p>
    <br/>
    <p style="margin: 0; padding: 0;">
    This workshop (KEIR @ ECIR 2025) serves as a platform to discuss innovative approaches that integrate external knowledge, aiming to enhance the effectiveness of information retrieval in a rapidly evolving technological landscape. Our goal is to bring together researchers from academia and industry to explore various aspects of knowledge-enhanced information retrieval.</p>
    


    <a id="cfp"><h2>Call for Papers</h2></a>
    <p style="margin: 0; padding: 0;">
    We invite researchers to submit their latest work to the KEIR@ECIR 2025 workshop on various aspects of knowledge-enhanced information retrieval, including models, techniques, data collection, and evaluation methodologies. Topics covered will include, but are not limited to:</p>
    <br/>
    <ul class="custom-bullets">
        <li>Knowledge-enhanced information retrieval models
        </li>
        <li>Knowledge-enhanced approaches for query processing, including query pars-
ing, query expansion, relevance feedback, and query reformulation
        </li>
        <li>Knowledge-enhanced recommendation models
        </li>
	<li>Knowledge-enhanced language models for retrieval
        </li>
	<li>Data augmentation for knowledge-enhanced information retrieval
        </li>
	<li>Large language model enhanced information retrieval
        </li>
	<li>Data collection for knowledge-enhanced information retrieval
        </li>
	<li>Knowledge-enhanced retrieval-augmented generation models
        </li>
	<li>Knowledge-aware fine-tuning and optimization methods for large language models tailored to IR systems
        </li>
	<li>Applications of knowledge-enhanced retrievals, such as dialogue systems, question answering, summarisation, and other domain-specific applications
        </li>
	<li>Evaluation methodologies for knowledge-enhanced retrieval
        </li>
	<li>The interpretability and analysis of knowledge-enhanced models for IR, including potential biases, fairness, and ethical considerations
        </li>	
    </ul>
    
    <section id="submission-guidelines">
        <h3>Submission Guidelines</h3>
        <p style="margin: 0; padding: 0;">We invite authors to submit papers written in English. Submissions may range in length from a minimum of 6 pages to a maximum of 12 pages; however, references and supplementary materials may exceed this page count without limitation. In order to facilitate a double-blind review process, authors must ensure that submissions are fully anonymized. Please note that we do not impose a specific anonymity period prior to submission. </p>
	<br/>
	<p style="margin: 0; padding: 0;">
The papers (.pdf format) should be submitted using the EasyChair submission system at <a href="" target="_blank">""</a>. Authors should consult Springer’s authors’ guidelines and use their proceedings templates to prepare the submission. The Microsoft Word and LaTeX versions of the template can be found at <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines</a>. Submissions to KEIR@ECIR2024 will be peer-reviewed on the basis of technical quality, relevance to workshop topics, originality, significance, clarity, etc.</p>
        <br/>
	<p style="margin: 0; padding: 0;"> We accept submissions of the following types:</p>
	<ul class="custom-bullets">
		<li>Original work that is not published or submitted elsewhere.
        </li>
		<li>Work that is submitted elsewhere and is still under review. In this case, the authors should make sure that they are not violating the submission guidelines and anonymity requirements of the other venue(s).
        </li>
		<li>Work that has been rejected at ECIR 2024.
        </li>
	</ul>
    
        <h3>Important dates</h3>
        <ul class="custom-bullets">
	    <li>Submission Deadline: <strong>January 12, 2025 </strong></li>
            <li>Acceptance Notification: <strong>February 16, 2025</strong></li>
            <li>ECIR Workshop: <strong>April 10, 2025</strong></li>
            <li>Deadlines refer to 23:59 (11:59 pm) in the AoE (Anywhere on Earth) time zone.</li>
        </ul>
        
        <!-- <section id="topics">
            <h3>Topics</h3>
            The workshop will concentrate on various aspects of knowledge-enhanced information retrieval, including models, techniques, data collection, and evaluation methodologies. Topics covered will include, but are not limited to:

        </section> -->
        
    <!-- <a id="accepted_papers"><h2>Accepted Papers</h2></a>
    <div class="paper">
        <p>
            <span class="authors">Kang Zhao, Xinyu Zhao, Zhipeng Jin, Yi Yang, Xuewu Jiao, Wen Tao, Yafei Li, Cong Han, Shuanglong Li, and Lin Liu.</span>
            <span class="title"> <strong>Image Captioning for Baidu Ad Image Generation with Multi-Stage Refinements.</strong></span>
        </p>
    </div>
    <div class="paper">
        <p>
            <span class="authors">Jing Zhu, Xiang Song, Vassilis Ioannidis, Danai Koutra, and Christos Faloutsos.</span>
            <span class="title"><strong><a href="https://www.amazon.science/publications/touchup-g-improving-feature-representation-through-graph-centric-finetuning">Improving Feature Representation through Graph-Centric Finetuning.</a></strong></span>
        </p>
    </div>
    <div class="paper">
        <p>
            <span class="authors">Wenliang Zhong, Wenyi Wu, Qi Li, Rob Barton, Boxin Du, Karim Bouyarmane, Shioulin Sam, Ismail Tutar, and Junzhou Huang.</span>
            <span class="title"><strong><a href="https://arxiv.org/abs/2406.02987">Enhancing Multimodal Large Language Models with Multi-instance Visual Prompt Generator for Visual Representation Enrichment.</a></strong></span>
        </p>
    </div>
    <div class="paper">
        <p>
            <span class="authors">Kevin Dela Rosa.</span>
            <span class="title"><strong><a href="https://arxiv.org/abs/2405.17706">Video Enriched Retrieval Augmented Generation Using Aligned Video Captions.</a></strong></span>
        </p>
    </div>    
    <div class="paper">
        <p>
            <span class="authors">Mingwei Tang, Meng Liu, Hong Li, Junjie Yang, Chenglin Wei, Boyang Li, Dai Li, Rengan Xu, Yifan Xu, Zehua Zhang, Xiangyu Wang, Linfeng Liu, Yuelei Xie, Chengye Liu, Labib Fawaz, Li Li, Hongnan Wang, Bill Zhu, and Sri Reddy.</span>
            <span class="title"><strong><a href="https://arxiv.org/abs/2406.05898v2">Async Learned User Embeddings for Ads Delivery Optimization.</a></strong></span>
        </p>
    </div>    
    <div class="paper">
        <p>
            <span class="authors">Sarthak Srivastava, and Kathy Wu.</span>
            <span class="title"><strong><a href="https://www.amazon.science/publications/vision-language-understanding-in-hyperbolic-space">Vision-Language Understanding in Hyperbolic Space.</a></strong></span>
        </p>
    </div> -->

	    
<a id="program"><h2>Program</h2></a>
<!-- <br /> -->
TBA
<!-- <table border="1" cellpadding="5" cellspacing="0" style="border-collapse: collapse;">
  <tr>
    <th><strong>Time</strong></th>
    <th><strong>Activity</strong></th>
    <th><strong>Host</strong></th>
  </tr>
  <tr>
    <td>9:00 AM - 9:05 AM</td>
    <td>Opening Remarks</td>
    <td>Doug Gray</td>
  </tr>
  <tr>
    <td>9:05 AM - 9:35 AM</td>
    <td>Keynote Address by Hamed Zamani</td>
    <td>Doug Gray</td>
  </tr>
  <tr>
    <td>9:35 AM - 10:35 AM</td>
    <td>Oral Presentations</td>
    <td>Xinliang Zhu</td>
  </tr>
  <tr>
    <td>10:35 AM - 10:45 AM</td>
    <td>Coffee Break</td>
    <td>-</td>
  </tr>
  <tr>
    <td>10:45 AM - 11:15 AM</td>
    <td>Keynote Address by Dinesh Manocha</td>
    <td>Arnab Dhua</td>
  </tr>
  <tr>
    <td>11:15 AM - 11:45 AM</td>
    <td>Panel Discussion</td>
    <td>Arnab Dhua</td>
  </tr>
  <tr>
    <td>11:45 AM - 11:50 AM</td>
    <td>Closing Remarks</td>
    <td>Xinliang Zhu</td>
  </tr>
  <tr>
    <td>11:50 AM - 12:15 PM</td>
    <td>Networking</td>
    <td>-</td>
  </tr>
</table> -->


<a id="speakers"><h2>Speakers</h2></a>
TBA

<!-- <ul>
<li>
	<a class="profile-link" href="https://www.cics.umass.edu/people/zamani-hamed">
	<img class="profile-image" src="./assets/Panelist/hamed.jpg" alt="Hamed Zamani">  Hamed Zamani, Associate Professor, University of Massachusetts Amherst</a>
</li>
	<p> Abstract: Information access systems, such as search engines and recommender systems, have long supported people in accomplishing a wide range of tasks. In this talk, I will discuss how one can broaden the scope of users of information access systems to include task-driven machines, such as generative AI models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. I will describe a generic retrieval-enhanced machine learning (REML) framework and connect this framework with the information retrieval literature. I will next introduce our recent implementations of REML for various language and vision tasks. Finally, I will discuss open problems in this area for future explorations.  </p>
<br/>

<li>
	<a class="profile-link" href="https://www.cs.umd.edu/people/dmanocha">
	<img class="profile-image" src="./assets/Panelist/dinesh.jpg" alt="Dinesh Manocha"> Dinesh Manocha, Professor, University of Maryland
	</a>
</li>
	<p>Abstract: Perceiving and understanding non-speech sounds and non-verbal speech are essential for making informed decisions that facilitate our interactions with our surroundings. Audio is a crucial modality, offering rich, contextual information that complements visual and textual data, thereby enhancing the capabilities of AI systems. In this talk, we will highlight the significance of audio as an integral component in developing the next generation of intelligent AI agents. We will highlight why audio is an indispensable modality for AI, highlighting how humans naturally and extensively rely on auditory cues to navigate and comprehend the physical and virtual worlds.  Understanding the auditory signals is fundamental to creating AI systems that can interact with the world in a more human-like and intuitive manner. Next, we will discuss how contemporary AI systems are beginning to integrate audio perception with other modalities to achieve more holistic and accurate environmental awareness. We will describe key advancements and methodologies that enable these multimodal integrations, focusing on the role of audio encoders and large language models (LLMs) in this synergy. Finally, we will address the open challenges and future directions in the field of audio question answering and multimodal AI.</p>
</ul> -->

<!-- <a id="panel"><h2>Panel</h2></a>
<ul>
        <li>
            <a class="profile-link" href="https://www.linkedin.com/in/bindalanuj/">
                <img class="profile-image" src="./assets/Panelist/anuj.jpeg" alt="Anuj Bindal"> Anuj Bindal, Director of Visual Shopping, Amazon
            </a>
	</li>
	<li>
	    <a class="profile-link" href="https://ischool.uw.edu/people/faculty/profile/chirags">
		<img class="profile-image" src="./assets/Panelist/chirag.jpeg" alt="Chirag Shah"> Chirag Shah, Professor, University of Washington
	   </a>
	</li>
	<li>
	   <a class="profile-link" href="https://www.cs.umd.edu/people/dmanocha">
		<img class="profile-image" src="./assets/Panelist/dinesh.jpg" alt="Dinesh Manocha"> Dinesh Manocha, Professor, University of Maryland
	   </a>
	</li>
	<li>
	   <a class="profile-link" href="https://www.cics.umass.edu/people/zamani-hamed">
	   	<img class="profile-image" src="./assets/Panelist/hamed.jpg" alt="Hamed Zamani">  Hamed Zamani,  Associate Professor, UMass Amherst
	   </a>
        </li>
</ul> -->
    

 <a id="organizers"><h2>Organizers</h2></a>
    <ul class="custom-bullets">
        <li>
            <a class="profile-link" href="https://wzh-nlp.github.io/">
                <!-- <img class="profile-image" src="./assets/Organizer/xinliang.jpg" alt="Xinliang Zhu"> -->
                 Zihan Wang, University of Amsterdam
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://scholar.google.com/citations?user=LOWJnPsAAAAJ">
                <!-- <img class="profile-image" src="./assets/Organizer/arnab.jpeg" alt="Arnab Dhua">  -->
                Jinyuan Fang, University of Glasgow
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://giacomofrisoni.github.io/">
                <!-- <img class="profile-image" src="./assets/Organizer/doug.jpeg" alt="Douglas Gray">  -->
                Giacomo Frisoni, University of Bologna
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://www.linkedin.com/in/zhuyundai/">
                <!-- <img class="profile-image" src="./assets/Organizer/zeki.jpeg" alt="I. Zeki Yalniz">  -->
                Zhuyun Dai, Google DeepMind
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://mengzaiqiao.github.io/">
                <!-- <img class="profile-image" src="./assets/Organizer/tan.jpeg" alt="Tan Yu">  -->
                Zaiqiao Meng, University of Glasgow
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://www.unibo.it/sitoweb/gianluca.moro">
                <!-- <img class="profile-image" src="./assets/Organizer/mohamed.png" alt="Mohamed H. Elhoseiny">  -->
                Gianluca Moro, University of Bologna
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://sites.google.com/site/emineyilmaz/">
                <!-- <img class="profile-image" src="./assets/Organizer/bryan.jpg" alt="Bryan A. Plummer">  -->
                Emine Yilmaz, University College London
            </a>
        </li>
    </ul>
</body>
</html>

</body>
</html>
